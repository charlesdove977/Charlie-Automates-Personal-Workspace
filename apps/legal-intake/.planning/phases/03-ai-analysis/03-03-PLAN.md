---
phase: 03-ai-analysis
plan: 03
type: execute
---

<objective>
Integrate extraction and analysis into an automated pipeline triggered on case submission.

Purpose: Automatically process new cases end-to-end without manual API calls.
Output: Complete AI analysis pipeline with status tracking and processing state management.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan context:
@.planning/phases/03-ai-analysis/03-01-SUMMARY.md
@.planning/phases/03-ai-analysis/03-02-SUMMARY.md

# Key files:
@src/lib/extraction.ts
@src/lib/analysis.ts
@src/app/api/cases/submit/route.ts
@prisma/schema.prisma

**Tech stack available:** Next.js 16.1.1, React 19, Prisma 7
**Established patterns:** API response utilities, Prisma singleton, multi-tenant by firmId

**Constraining decisions:**
- Case starts with PENDING status
- briefJson populated by AI analysis
- fitScore populated by analysis (basic version)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add processing status to Case model</name>
  <files>prisma/schema.prisma, src/lib/db.ts</files>
  <action>
1. Add processingStatus enum to schema.prisma:
   ```prisma
   enum ProcessingStatus {
     QUEUED
     EXTRACTING
     ANALYZING
     COMPLETE
     FAILED
   }
   ```

2. Add fields to Case model:
   ```prisma
   processingStatus ProcessingStatus @default(QUEUED)
   processingError  String?
   processedAt      DateTime?
   ```

3. Run prisma db push to update schema:
   ```bash
   pnpm prisma db push
   ```

4. Regenerate Prisma client:
   ```bash
   pnpm prisma generate
   ```
  </action>
  <verify>Schema updated, ProcessingStatus enum available in Prisma client</verify>
  <done>Case model has processing status tracking fields</done>
</task>

<task type="auto">
  <name>Task 2: Create unified processing service</name>
  <files>src/lib/processing.ts</files>
  <action>
Create src/lib/processing.ts that orchestrates the full pipeline:

1. Export async function processCase(caseId: string): Promise<void>
   - This is the main entry point for processing

2. Implementation:
   ```typescript
   async function processCase(caseId: string): Promise<void> {
     const prisma = db()

     try {
       // Mark as EXTRACTING
       await prisma.case.update({
         where: { id: caseId },
         data: { processingStatus: 'EXTRACTING' }
       })

       // Get case with documents
       const caseData = await prisma.case.findUnique({
         where: { id: caseId },
         include: { documents: true }
       })

       if (!caseData) throw new Error('Case not found')

       // Extract text from each document
       for (const doc of caseData.documents) {
         if (!doc.extractedText) {
           const result = await extractTextFromDocument(doc.storageUrl, doc.mimeType)
           if (result.text) {
             await prisma.document.update({
               where: { id: doc.id },
               data: { extractedText: result.text }
             })
           }
         }
       }

       // Mark as ANALYZING
       await prisma.case.update({
         where: { id: caseId },
         data: { processingStatus: 'ANALYZING' }
       })

       // Reload documents with extracted text
       const updatedCase = await prisma.case.findUnique({
         where: { id: caseId },
         include: { documents: true }
       })

       // Run AI analysis
       const docsWithText = updatedCase.documents.filter(d => d.extractedText)

       if (docsWithText.length > 0) {
         const brief = await analyzeCase(docsWithText, updatedCase.caseType)

         // Calculate basic fit score (count of high-importance facts)
         const fitScore = calculateBasicFitScore(brief)

         await prisma.case.update({
           where: { id: caseId },
           data: {
             briefJson: brief,
             fitScore,
             processingStatus: 'COMPLETE',
             processedAt: new Date()
           }
         })
       } else {
         // No text to analyze
         await prisma.case.update({
           where: { id: caseId },
           data: {
             processingStatus: 'COMPLETE',
             processedAt: new Date(),
             processingError: 'No extractable text in documents'
           }
         })
       }

     } catch (error) {
       console.error('Processing failed:', error)
       await prisma.case.update({
         where: { id: caseId },
         data: {
           processingStatus: 'FAILED',
           processingError: error.message
         }
       })
       throw error
     }
   }
   ```

3. Add helper function calculateBasicFitScore(brief: CaseBrief): number
   - Count high-importance key facts
   - Subtract points for high-severity red flags
   - Return score 0-100
   - This is a basic implementation; Phase 5 will add firm-specific criteria

4. Add logging throughout for debugging
  </action>
  <verify>processCase function compiles and handles full pipeline</verify>
  <done>Unified processing service orchestrates extraction → analysis flow</done>
</task>

<task type="auto">
  <name>Task 3: Trigger processing on case submission</name>
  <files>src/app/api/cases/submit/route.ts</files>
  <action>
Update the case submission endpoint to trigger processing:

1. After successfully creating Case and Document records:
   - Do NOT await processCase() - let it run asynchronously
   - Use a fire-and-forget pattern to avoid blocking the response

2. Implementation pattern:
   ```typescript
   // After transaction completes successfully:
   // Fire and forget - don't await
   processCase(newCase.id).catch(error => {
     console.error('Background processing failed:', error)
   })
   ```

3. Alternative: If background processing is unreliable in serverless:
   - Return caseId to client
   - Let client poll for status
   - Or use a separate processing endpoint

4. Update response to include processingStatus:
   ```json
   {
     "caseId": "...",
     "message": "Case submitted successfully",
     "processingStatus": "QUEUED"
   }
   ```

Note: For serverless environments like Vercel, the fire-and-forget approach may timeout.
Consider using Vercel Functions background tasks or a queue service for production.
For MVP, the synchronous approach with longer timeout is acceptable.
  </action>
  <verify>Case submission triggers processing pipeline</verify>
  <done>Processing automatically starts when case is submitted</done>
</task>

<task type="auto">
  <name>Task 4: Create processing status API endpoint</name>
  <files>src/app/api/cases/[caseId]/status/route.ts</files>
  <action>
Create GET endpoint to check processing status:

1. Create src/app/api/cases/[caseId]/status/route.ts:
   - Validate caseId parameter
   - Look up Case using Prisma
   - Return 404 if not found

2. Return processing status:
   ```json
   {
     "caseId": "...",
     "status": "ANALYZING",
     "error": null,
     "processedAt": null,
     "hasBrief": false
   }
   ```

3. When status is COMPLETE and hasBrief is true, client knows brief is ready

4. Add cache-control: no-store to prevent caching
  </action>
  <verify>GET /api/cases/[caseId]/status returns current processing state</verify>
  <done>Status endpoint allows clients to poll for processing completion</done>
</task>

<task type="auto">
  <name>Task 5: Create manual reprocess endpoint</name>
  <files>src/app/api/cases/[caseId]/reprocess/route.ts</files>
  <action>
Create POST endpoint to manually trigger reprocessing:

1. Create src/app/api/cases/[caseId]/reprocess/route.ts:
   - Validate caseId parameter
   - Look up Case using Prisma
   - Return 404 if not found

2. Reset processing state:
   - Set processingStatus to QUEUED
   - Clear processingError
   - Clear briefJson (to force fresh analysis)

3. Trigger processCase() (fire and forget)

4. Return response:
   ```json
   {
     "message": "Reprocessing started",
     "caseId": "...",
     "processingStatus": "QUEUED"
   }
   ```

5. Use cases:
   - Retry after FAILED status
   - Regenerate brief with updated prompt
   - Debug processing issues
  </action>
  <verify>POST /api/cases/[caseId]/reprocess triggers fresh processing</verify>
  <done>Manual reprocessing endpoint for retry and debugging</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete AI analysis pipeline with automatic processing and status tracking</what-built>
  <how-to-verify>
    1. Ensure environment variables set:
       - ANTHROPIC_API_KEY
       - Supabase credentials
    2. Start dev server: `pnpm dev`
    3. Submit a new case through the intake form with documents
    4. Check processing status:
       ```bash
       curl http://localhost:3000/api/cases/[new-case-id]/status
       ```
    5. Wait for status to reach COMPLETE (may take 30-60 seconds)
    6. Verify in database:
       - Case.processingStatus = COMPLETE
       - Case.briefJson populated with structured brief
       - Case.fitScore calculated
       - Document.extractedText populated
    7. Test reprocess endpoint:
       ```bash
       curl -X POST http://localhost:3000/api/cases/[case-id]/reprocess
       ```
    8. Verify status resets and processing runs again
  </how-to-verify>
  <resume-signal>Type "approved" to continue, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pnpm build` succeeds without errors
- [ ] `pnpm lint` passes
- [ ] ProcessingStatus enum working in Prisma
- [ ] Case submission triggers automatic processing
- [ ] Status endpoint returns current processing state
- [ ] Reprocess endpoint works for retries
- [ ] End-to-end: submit → extract → analyze → complete
- [ ] Error handling populates processingError
</verification>

<success_criteria>
- Full processing pipeline automated
- Processing status visible via API
- Error cases handled gracefully
- Reprocessing available for retries
- Phase 3 complete: AI analysis engine functional
- Ready for Phase 4: Attorney Dashboard
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-analysis/03-03-SUMMARY.md` following the summary template.

Include in summary:
- Phase 3 complete status
- Full pipeline flow documented
- Performance characteristics (processing time)
- Any production considerations noted
- Ready for Phase 4
</output>
